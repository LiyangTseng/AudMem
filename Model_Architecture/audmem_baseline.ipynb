{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8fbcb31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/lab812/anaconda3/envs/musicmem/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/lab812/.local/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/lab812/.local/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/lab812/.local/lib/python3.6/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/lab812/anaconda3/envs/musicmem/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05dc620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5650ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7) \n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(7)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1e203535",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d32a1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = \"data/labels/track_memorability_scores_beta.csv\"\n",
    "df = pd.read_csv(label_file)\n",
    "labels = df.score.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7645c5",
   "metadata": {},
   "source": [
    "### chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "affc383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_dir = \"baseline_representation/chroma\"\n",
    "chroma_data = []\n",
    "\n",
    "# append chroma according to list to label's order\n",
    "for track in df.track.values:\n",
    "    chroma_path = os.path.join(chroma_dir, track.replace(\".wav\", \".npy\"))\n",
    "    chroma = np.load(chroma_path)\n",
    "    chroma_data.append(chroma)\n",
    "# convert data to np array\n",
    "chroma_data = np.array(chroma_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c98506ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 72)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9435ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4811eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:11,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlations mean: 0.04964517963527892, std: 0.24378401210638145\n",
      "loss mean: 0.03579263109713793, std: 0.009373647509314638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correlations, losses = [], []\n",
    "# K-fold Cross Validation model evaluation\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "for train, test in tqdm(kfold.split(chroma_data, labels)):\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(64, input_dim=chroma_data.shape[1], activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.0001))\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "    \n",
    "    train_features = chroma_data[train]\n",
    "    train_features = StandardScaler().fit_transform(train_features)\n",
    "    # Fit data to model\n",
    "    history = model.fit(train_features,\n",
    "                        labels[train],\n",
    "                        validation_split = 0.1,\n",
    "                        batch_size=16, \n",
    "                        epochs=100,\n",
    "                       verbose=0,\n",
    "                       callbacks=[callback])\n",
    "\n",
    "    test_features = chroma_data[test]\n",
    "    test_features = StandardScaler().fit_transform(test_features)\n",
    "    pred = model.predict(test_features)\n",
    "    correlations.append(stats.spearmanr(pred, labels[test])[0])\n",
    "    losses.append(model.evaluate(test_features, labels[test], verbose=0))\n",
    "\n",
    "print(\"correlations mean: {}, std: {}\".format(np.array(correlations).mean(), np.array(correlations).std()))\n",
    "print(\"loss mean: {}, std: {}\".format(np.array(losses).mean(), np.array(losses).std()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5295dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations mean: 0.04964517963527892, std: 0.24378401210638145\n",
    "# loss mean: 0.03579263109713793, std: 0.009373648300209108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28c805",
   "metadata": {},
   "source": [
    "### mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2adcc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_dir = \"baseline_representation/mfcc\"\n",
    "mfcc_data = []\n",
    "\n",
    "# append mfcc according to list to label's order\n",
    "for track in df.track.values:\n",
    "    mfcc_path = os.path.join(mfcc_dir, track.replace(\".wav\", \".npy\"))\n",
    "    mfcc = np.load(mfcc_path)\n",
    "    mfcc_data.append(mfcc)\n",
    "# convert data to np array\n",
    "mfcc_data = np.array(mfcc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39b3523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 120)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "306f43f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:12,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlations mean: 0.2025604399531912, std: 0.22066895748641988\n",
      "loss mean: 0.03040823470801115, std: 0.00535532887904776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correlations, losses = [], []\n",
    "# K-fold Cross Validation model evaluation\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "for train, test in tqdm(kfold.split(mfcc_data, labels)):\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(64, input_dim=mfcc_data.shape[1], activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.0001))\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "    \n",
    "    # Fit data to model\n",
    "    train_features = mfcc_data[train]\n",
    "    train_features = StandardScaler().fit_transform(train_features)\n",
    "    # Fit data to model\n",
    "    history = model.fit(train_features,\n",
    "                        labels[train],\n",
    "                        validation_split = 0.1,\n",
    "                        batch_size=16, \n",
    "                        epochs=100,\n",
    "                       verbose=0,\n",
    "                       callbacks=[callback])\n",
    "\n",
    "    test_features = mfcc_data[test]\n",
    "    test_features = StandardScaler().fit_transform(test_features)\n",
    "    pred = model.predict(test_features)\n",
    "    correlations.append(stats.spearmanr(pred, labels[test])[0])\n",
    "    losses.append(model.evaluate(test_features, labels[test], verbose=0))\n",
    "\n",
    "print(\"correlations mean: {}, std: {}\".format(np.array(correlations).mean(), np.array(correlations).std()))\n",
    "print(\"loss mean: {}, std: {}\".format(np.array(losses).mean(), np.array(losses).std()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4f4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations mean: 0.2025604399531912, std: 0.22066895748641988\n",
    "# loss mean: 0.03040823470801115, std: 0.00535532887904776"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7816717",
   "metadata": {},
   "source": [
    "### choi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c887795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "choi_dir = \"baseline_representation/choi\"\n",
    "choi_data = []\n",
    "\n",
    "# append mfcc according to list to label's order\n",
    "for track in df.track.values:\n",
    "    choi_path = os.path.join(choi_dir, track.replace(\".wav\", \".npy\"))\n",
    "    choi = np.load(choi_path)\n",
    "    choi_data.append(choi)\n",
    "# convert data to np array\n",
    "choi_data = np.array(choi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e00c8fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 160)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choi_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "941f19a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:12,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlations mean: 0.10312894005227528, std: 0.2271080382532025\n",
      "loss mean: 0.034260318800807, std: 0.011052766953853571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correlations, losses = [], []\n",
    "# K-fold Cross Validation model evaluation\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "for train, test in tqdm(kfold.split(choi_data, labels)):\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(100, input_dim=choi_data.shape[1], activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "    \n",
    "    # Fit data to model\n",
    "    train_features = choi_data[train]\n",
    "    train_features = StandardScaler().fit_transform(train_features)\n",
    "    # Fit data to model\n",
    "    history = model.fit(train_features,\n",
    "                        labels[train],\n",
    "                        validation_split = 0.1,\n",
    "                        batch_size=16, \n",
    "                        epochs=100,\n",
    "                       verbose=0,\n",
    "                       callbacks=[callback])\n",
    "\n",
    "    test_features = choi_data[test]\n",
    "    test_features = StandardScaler().fit_transform(test_features)\n",
    "    pred = model.predict(test_features)\n",
    "    correlations.append(stats.spearmanr(pred, labels[test])[0])\n",
    "    losses.append(model.evaluate(test_features, labels[test], verbose=0))\n",
    "\n",
    "print(\"correlations mean: {}, std: {}\".format(np.array(correlations).mean(), np.array(correlations).std()))\n",
    "print(\"loss mean: {}, std: {}\".format(np.array(losses).mean(), np.array(losses).std()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72a0a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations mean: 0.10312894005227528, std: 0.2271080382532025\n",
    "# loss mean: 0.034260318800807, std: 0.011052766953853571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4f0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7da9e240",
   "metadata": {},
   "source": [
    "### clmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f43338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clmr_dir = \"baseline_representation/clmr\"\n",
    "clmr_data = []\n",
    "\n",
    "# append mfcc according to list to label's order\n",
    "for track in df.track.values:\n",
    "    clmr_path = os.path.join(clmr_dir, track.replace(\".wav\", \".npy\"))\n",
    "    clmr = np.load(clmr_path)\n",
    "    clmr_data.append(clmr)\n",
    "# convert data to np array\n",
    "clmr_data = np.array(clmr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16c6d2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 512)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clmr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36c4868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:16,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlations mean: 0.052647734917081634, std: 0.19079969278339423\n",
      "loss mean: 0.0341348934918642, std: 0.00996552185332706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correlations, losses = [], []\n",
    "# K-fold Cross Validation model evaluation\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "for train, test in tqdm(kfold.split(clmr_data, labels)):\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(100, input_dim=clmr_data.shape[1], activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "    \n",
    "    # Fit data to model\n",
    "    train_features = clmr_data[train]\n",
    "    train_features = StandardScaler().fit_transform(train_features)\n",
    "    # Fit data to model\n",
    "    history = model.fit(train_features,\n",
    "                        labels[train],\n",
    "                        validation_split = 0.1,\n",
    "                        batch_size=16, \n",
    "                        epochs=100,\n",
    "                       verbose=0,\n",
    "                       callbacks=[callback])\n",
    "\n",
    "    test_features = clmr_data[test]\n",
    "    test_features = StandardScaler().fit_transform(test_features)\n",
    "    pred = model.predict(test_features)\n",
    "    correlations.append(stats.spearmanr(pred, labels[test])[0])\n",
    "    losses.append(model.evaluate(test_features, labels[test], verbose=0))\n",
    "\n",
    "print(\"correlations mean: {}, std: {}\".format(np.array(correlations).mean(), np.array(correlations).std()))\n",
    "print(\"loss mean: {}, std: {}\".format(np.array(losses).mean(), np.array(losses).std()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "197635a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations mean: 0.052647734917081634, std: 0.19079969278339423\n",
    "# loss mean: 0.0341348934918642, std: 0.00996552185332706"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5d9ee",
   "metadata": {},
   "source": [
    "### MusiCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9487f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "musicnn_dir = \"baseline_representation/musicnn\"\n",
    "musicnn_data = []\n",
    "\n",
    "# append mfcc according to list to label's order\n",
    "for track in df.track.values:\n",
    "    musicnn_path = os.path.join(musicnn_dir, track.replace(\".wav\", \".npy\"))\n",
    "    musicnn = np.load(musicnn_path)\n",
    "    musicnn_data.append(musicnn)\n",
    "# convert data to np array\n",
    "musicnn_data = np.array(musicnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "129c0c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 4194)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "musicnn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6543f563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:19,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlations mean: 0.23706924302250304, std: 0.14870892357692989\n",
      "loss mean: 0.02985965069383383, std: 0.008997555588942796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correlations, losses = [], []\n",
    "# K-fold Cross Validation model evaluation\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "for train, test in tqdm(kfold.split(musicnn_data, labels)):\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(100, input_dim=musicnn_data.shape[1], activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "    \n",
    "    # Fit data to model\n",
    "    train_features = musicnn_data[train]\n",
    "    train_features = StandardScaler().fit_transform(train_features)\n",
    "    # Fit data to model\n",
    "    history = model.fit(train_features,\n",
    "                        labels[train],\n",
    "                        validation_split = 0.1,\n",
    "                        batch_size=16, \n",
    "                        epochs=100,\n",
    "                       verbose=0,\n",
    "                       callbacks=[callback])\n",
    "\n",
    "    test_features = musicnn_data[test]\n",
    "    test_features = StandardScaler().fit_transform(test_features)\n",
    "    pred = model.predict(test_features)\n",
    "    correlations.append(stats.spearmanr(pred, labels[test])[0])\n",
    "    losses.append(model.evaluate(test_features, labels[test], verbose=0))\n",
    "\n",
    "print(\"correlations mean: {}, std: {}\".format(np.array(correlations).mean(), np.array(correlations).std()))\n",
    "print(\"loss mean: {}, std: {}\".format(np.array(losses).mean(), np.array(losses).std()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04369af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations mean: 0.23706924302250304, std: 0.14870892357692989\n",
    "# loss mean: 0.02985965069383383, std: 0.008997555588942796"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d87300",
   "metadata": {},
   "source": [
    "### PANNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9487f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "panns_dir = \"baseline_representation/panns\"\n",
    "panns_data = []\n",
    "\n",
    "# append mfcc according to list to label's order\n",
    "for track in df.track.values:\n",
    "    panns_path = os.path.join(panns_dir, track.replace(\".wav\", \".npy\"))\n",
    "    panns = np.load(panns_path)\n",
    "    panns = panns.flatten()\n",
    "    panns_data.append(panns)\n",
    "# convert data to np array\n",
    "panns_data = np.array(panns_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "129c0c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 2048)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panns_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6543f563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:15,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlations mean: 0.1606687015090455, std: 0.2054919432925273\n",
      "loss mean: 0.03130340985953808, std: 0.01036696120662695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correlations, losses = [], []\n",
    "# K-fold Cross Validation model evaluation\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "for train, test in tqdm(kfold.split(panns_data, labels)):\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(100, input_dim=panns_data.shape[1], activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "    \n",
    "    # Fit data to model\n",
    "    train_features = panns_data[train]\n",
    "    train_features = StandardScaler().fit_transform(train_features)\n",
    "    # Fit data to model\n",
    "    history = model.fit(train_features,\n",
    "                        labels[train],\n",
    "                        validation_split = 0.1,\n",
    "                        batch_size=16, \n",
    "                        epochs=100,\n",
    "                       verbose=0,\n",
    "                       callbacks=[callback])\n",
    "\n",
    "    test_features = panns_data[test]\n",
    "    test_features = StandardScaler().fit_transform(test_features)\n",
    "    pred = model.predict(test_features)\n",
    "    correlations.append(stats.spearmanr(pred, labels[test])[0])\n",
    "    losses.append(model.evaluate(test_features, labels[test], verbose=0))\n",
    "\n",
    "print(\"correlations mean: {}, std: {}\".format(np.array(correlations).mean(), np.array(correlations).std()))\n",
    "print(\"loss mean: {}, std: {}\".format(np.array(losses).mean(), np.array(losses).std()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04369af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations mean: 0.1606687015090455, std: 0.2054919432925273\n",
    "# loss mean: 0.03130340985953808, std: 0.01036696120662695"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d0738b759428c454e0fd31be71d48c520920c4bd96f0a3057f017c32e697ea3"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('musicmem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
